Data preparation in progress for the kiba dataset...
[[11.1                nan         nan ...         nan         nan
          nan]
 [11.1                nan         nan ...         nan         nan
          nan]
 [12.1        11.99999842         nan ...         nan 14.40016227
  11.30000024]
 ...
 [        nan         nan         nan ...         nan         nan
  12.637602  ]
 [        nan         nan         nan ...         nan         nan
          nan]
 [        nan         nan         nan ...         nan         nan
          nan]]
/data/home/congn/MyCode/Experiment/MSGC-DTA/utils.py:90: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:641.)
  return torch.sparse.FloatTensor(indices, values, shape)
/data/conda/congn/envs/MSGC-DTA/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/data/home/congn/MyCode/Experiment/MSGC-DTA/utils.py:71: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)
  GCNData = DATA.Data(x=torch.Tensor(features), edge_index=torch.LongTensor(edge_index).transpose(1, 0))
(2111, 100)
(2111, 300)
(2111, 512)
(228, 100)
(228, 768)
(228, 1280)
Model preparation...
Start training...
Training on 98400 samples...
Total number of parameters: 4396164
Trainable parameters: 4396164
/data/conda/congn/envs/MSGC-DTA/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'dropout_adj' is deprecated, use 'dropout_edge' instead
  warnings.warn(out)
Train epoch: 1 [0/98400 (0%)]	Loss: 145.006439
Train epoch: 1 [5120/98400 (5%)]	Loss: 69.464287
Train epoch: 1 [10240/98400 (10%)]	Loss: 13.766346
Train epoch: 1 [15360/98400 (16%)]	Loss: 12.862296
Train epoch: 1 [20480/98400 (21%)]	Loss: 10.630259
Train epoch: 1 [25600/98400 (26%)]	Loss: 9.942131
Train epoch: 1 [30720/98400 (31%)]	Loss: 9.427253
Train epoch: 1 [35840/98400 (36%)]	Loss: 9.642294
Train epoch: 1 [40960/98400 (41%)]	Loss: 9.506771
Train epoch: 1 [46080/98400 (47%)]	Loss: 9.359335
Train epoch: 1 [51200/98400 (52%)]	Loss: 9.424996
Train epoch: 1 [56320/98400 (57%)]	Loss: 9.238688
Train epoch: 1 [61440/98400 (62%)]	Loss: 9.167009
Train epoch: 1 [66560/98400 (67%)]	Loss: 9.303068
Train epoch: 1 [71680/98400 (73%)]	Loss: 9.271836
Train epoch: 1 [76800/98400 (78%)]	Loss: 9.082100
Train epoch: 1 [81920/98400 (83%)]	Loss: 9.187538
Train epoch: 1 [87040/98400 (88%)]	Loss: 9.130909
Train epoch: 1 [92160/98400 (93%)]	Loss: 9.135981
Train epoch: 1 [97280/98400 (98%)]	Loss: 9.124765
Make prediction for 19683 samples...
result: (0.5419213, 0.21964620418291558)
Training on 98400 samples...
Total number of parameters: 4396164
Trainable parameters: 4396164
Train epoch: 2 [0/98400 (0%)]	Loss: 9.084092
Train epoch: 2 [5120/98400 (5%)]	Loss: 8.988132
Train epoch: 2 [10240/98400 (10%)]	Loss: 9.041595
Train epoch: 2 [15360/98400 (16%)]	Loss: 9.187377
Train epoch: 2 [20480/98400 (21%)]	Loss: 8.960174
Train epoch: 2 [25600/98400 (26%)]	Loss: 9.072054
Train epoch: 2 [30720/98400 (31%)]	Loss: 8.982030
Train epoch: 2 [35840/98400 (36%)]	Loss: 9.080122
Train epoch: 2 [40960/98400 (41%)]	Loss: 8.989402
Train epoch: 2 [46080/98400 (47%)]	Loss: 9.148659
Train epoch: 2 [51200/98400 (52%)]	Loss: 8.874394
Train epoch: 2 [56320/98400 (57%)]	Loss: 8.976648
Train epoch: 2 [61440/98400 (62%)]	Loss: 8.954982
Train epoch: 2 [66560/98400 (67%)]	Loss: 8.864719
Train epoch: 2 [71680/98400 (73%)]	Loss: 8.887296
Train epoch: 2 [76800/98400 (78%)]	Loss: 9.026367
Train epoch: 2 [81920/98400 (83%)]	Loss: 8.755985
Train epoch: 2 [87040/98400 (88%)]	Loss: 8.690894
Train epoch: 2 [92160/98400 (93%)]	Loss: 8.748923
Train epoch: 2 [97280/98400 (98%)]	Loss: 8.857995
Make prediction for 19683 samples...
result: (0.45667833, 0.342787241857624)
Training on 98400 samples...
Total number of parameters: 4396164
Trainable parameters: 4396164
Train epoch: 3 [0/98400 (0%)]	Loss: 8.822851
Train epoch: 3 [5120/98400 (5%)]	Loss: 8.653208
Train epoch: 3 [10240/98400 (10%)]	Loss: 8.713902
Train epoch: 3 [15360/98400 (16%)]	Loss: 8.657028
Train epoch: 3 [20480/98400 (21%)]	Loss: 8.772505
Train epoch: 3 [25600/98400 (26%)]	Loss: 8.675015
Train epoch: 3 [30720/98400 (31%)]	Loss: 8.712782
Train epoch: 3 [35840/98400 (36%)]	Loss: 8.580729
Train epoch: 3 [40960/98400 (41%)]	Loss: 8.651631
Train epoch: 3 [46080/98400 (47%)]	Loss: 8.547507
Train epoch: 3 [51200/98400 (52%)]	Loss: 8.668522
Train epoch: 3 [56320/98400 (57%)]	Loss: 8.544231
Train epoch: 3 [61440/98400 (62%)]	Loss: 8.611200
Train epoch: 3 [66560/98400 (67%)]	Loss: 8.569467
Train epoch: 3 [71680/98400 (73%)]	Loss: 8.524168
Train epoch: 3 [76800/98400 (78%)]	Loss: 8.669478
Train epoch: 3 [81920/98400 (83%)]	Loss: 8.466546
Train epoch: 3 [87040/98400 (88%)]	Loss: 8.422485
Train epoch: 3 [92160/98400 (93%)]	Loss: 8.435552
Train epoch: 3 [97280/98400 (98%)]	Loss: 8.409957
Make prediction for 19683 samples...
result: (0.4952416, 0.4184598877113674)
Training on 98400 samples...
Total number of parameters: 4396164
Trainable parameters: 4396164
Train epoch: 4 [0/98400 (0%)]	Loss: 8.584918
Train epoch: 4 [5120/98400 (5%)]	Loss: 8.488977
Train epoch: 4 [10240/98400 (10%)]	Loss: 8.379731
Train epoch: 4 [15360/98400 (16%)]	Loss: 8.389627
Train epoch: 4 [20480/98400 (21%)]	Loss: 8.511897
Train epoch: 4 [25600/98400 (26%)]	Loss: 8.442434
Train epoch: 4 [30720/98400 (31%)]	Loss: 8.517972
Train epoch: 4 [35840/98400 (36%)]	Loss: 8.338074
Train epoch: 4 [40960/98400 (41%)]	Loss: 8.387463
Train epoch: 4 [46080/98400 (47%)]	Loss: 8.279629
Train epoch: 4 [51200/98400 (52%)]	Loss: 8.308942
Train epoch: 4 [56320/98400 (57%)]	Loss: 8.305337
Train epoch: 4 [61440/98400 (62%)]	Loss: 8.261030
Train epoch: 4 [66560/98400 (67%)]	Loss: 8.355491
Train epoch: 4 [71680/98400 (73%)]	Loss: 8.207420
Train epoch: 4 [76800/98400 (78%)]	Loss: 8.269945
Train epoch: 4 [81920/98400 (83%)]	Loss: 8.262567
Train epoch: 4 [87040/98400 (88%)]	Loss: 8.327267
Train epoch: 4 [92160/98400 (93%)]	Loss: 8.231498
Train epoch: 4 [97280/98400 (98%)]	Loss: 8.296728
Make prediction for 19683 samples...
result: (0.3691469, 0.4663663069075257)
Training on 98400 samples...
Total number of parameters: 4396164
Trainable parameters: 4396164
Train epoch: 5 [0/98400 (0%)]	Loss: 8.252848
Train epoch: 5 [5120/98400 (5%)]	Loss: 8.220375
Train epoch: 5 [10240/98400 (10%)]	Loss: 8.218224
Train epoch: 5 [15360/98400 (16%)]	Loss: 8.247560
Train epoch: 5 [20480/98400 (21%)]	Loss: 8.164760
Train epoch: 5 [25600/98400 (26%)]	Loss: 8.203226
Train epoch: 5 [30720/98400 (31%)]	Loss: 8.197601
Train epoch: 5 [35840/98400 (36%)]	Loss: 8.278667
Train epoch: 5 [40960/98400 (41%)]	Loss: 8.222283
Train epoch: 5 [46080/98400 (47%)]	Loss: 8.220819
Train epoch: 5 [51200/98400 (52%)]	Loss: 8.217728
Train epoch: 5 [56320/98400 (57%)]	Loss: 8.190947
Train epoch: 5 [61440/98400 (62%)]	Loss: 8.197551
Train epoch: 5 [66560/98400 (67%)]	Loss: 8.116708
Train epoch: 5 [71680/98400 (73%)]	Loss: 8.100480
Train epoch: 5 [76800/98400 (78%)]	Loss: 8.190370
Train epoch: 5 [81920/98400 (83%)]	Loss: 8.108383
Train epoch: 5 [87040/98400 (88%)]	Loss: 8.123808
Train epoch: 5 [92160/98400 (93%)]	Loss: 8.123308
Train epoch: 5 [97280/98400 (98%)]	Loss: 8.124979
Make prediction for 19683 samples...
result: (0.4281883, 0.49242768620952926)
Training on 98400 samples...
Total number of parameters: 4396164
Trainable parameters: 4396164
Train epoch: 6 [0/98400 (0%)]	Loss: 8.139809
Train epoch: 6 [5120/98400 (5%)]	Loss: 8.109378
Train epoch: 6 [10240/98400 (10%)]	Loss: 8.140446
Train epoch: 6 [15360/98400 (16%)]	Loss: 8.131519
Train epoch: 6 [20480/98400 (21%)]	Loss: 8.007975
Train epoch: 6 [25600/98400 (26%)]	Loss: 8.029103
Train epoch: 6 [30720/98400 (31%)]	Loss: 8.066077
Train epoch: 6 [35840/98400 (36%)]	Loss: 8.087473
Train epoch: 6 [40960/98400 (41%)]	Loss: 8.054230
Train epoch: 6 [46080/98400 (47%)]	Loss: 7.990857
Train epoch: 6 [51200/98400 (52%)]	Loss: 8.017738
Train epoch: 6 [56320/98400 (57%)]	Loss: 8.105448
Train epoch: 6 [61440/98400 (62%)]	Loss: 8.015012
Train epoch: 6 [66560/98400 (67%)]	Loss: 8.003250
Train epoch: 6 [71680/98400 (73%)]	Loss: 7.970953
Train epoch: 6 [76800/98400 (78%)]	Loss: 8.081108
Train epoch: 6 [81920/98400 (83%)]	Loss: 8.027529
Train epoch: 6 [87040/98400 (88%)]	Loss: 7.972430
Train epoch: 6 [92160/98400 (93%)]	Loss: 8.094262
Train epoch: 6 [97280/98400 (98%)]	Loss: 7.975979
Make prediction for 19683 samples...
result: (0.36517847, 0.5201818859808359)
Training on 98400 samples...
Total number of parameters: 4396164
Trainable parameters: 4396164
Train epoch: 7 [0/98400 (0%)]	Loss: 8.109620
Train epoch: 7 [5120/98400 (5%)]	Loss: 8.051082
Train epoch: 7 [10240/98400 (10%)]	Loss: 7.970812
Train epoch: 7 [15360/98400 (16%)]	Loss: 7.949375
Train epoch: 7 [20480/98400 (21%)]	Loss: 7.947177
Train epoch: 7 [25600/98400 (26%)]	Loss: 7.947454
Train epoch: 7 [30720/98400 (31%)]	Loss: 7.920762
Train epoch: 7 [35840/98400 (36%)]	Loss: 7.957434
Train epoch: 7 [40960/98400 (41%)]	Loss: 7.941414
Train epoch: 7 [46080/98400 (47%)]	Loss: 7.837849
Train epoch: 7 [51200/98400 (52%)]	Loss: 7.920608
Train epoch: 7 [56320/98400 (57%)]	Loss: 7.971795
Train epoch: 7 [61440/98400 (62%)]	Loss: 7.903759
Train epoch: 7 [66560/98400 (67%)]	Loss: 7.813813
Train epoch: 7 [71680/98400 (73%)]	Loss: 7.872211
Train epoch: 7 [76800/98400 (78%)]	Loss: 7.893263
Train epoch: 7 [81920/98400 (83%)]	Loss: 7.867322
Train epoch: 7 [87040/98400 (88%)]	Loss: 7.899876
Train epoch: 7 [92160/98400 (93%)]	Loss: 7.860488
Train epoch: 7 [97280/98400 (98%)]	Loss: 7.815721
Make prediction for 19683 samples...
result: (0.3475187, 0.5261721109730788)
Training on 98400 samples...
Total number of parameters: 4396164
Trainable parameters: 4396164
Train epoch: 8 [0/98400 (0%)]	Loss: 7.816516
Train epoch: 8 [5120/98400 (5%)]	Loss: 7.880015
Train epoch: 8 [10240/98400 (10%)]	Loss: 7.834949
Train epoch: 8 [15360/98400 (16%)]	Loss: 7.910908
Train epoch: 8 [20480/98400 (21%)]	Loss: 7.895350
Train epoch: 8 [25600/98400 (26%)]	Loss: 7.812007
Train epoch: 8 [30720/98400 (31%)]	Loss: 7.925942
Train epoch: 8 [35840/98400 (36%)]	Loss: 7.868626
Train epoch: 8 [40960/98400 (41%)]	Loss: 8.016126
Train epoch: 8 [46080/98400 (47%)]	Loss: 7.959873
Train epoch: 8 [51200/98400 (52%)]	Loss: 8.009284
Train epoch: 8 [56320/98400 (57%)]	Loss: 7.763903
Train epoch: 8 [61440/98400 (62%)]	Loss: 7.751422
Train epoch: 8 [66560/98400 (67%)]	Loss: 7.837154
Train epoch: 8 [71680/98400 (73%)]	Loss: 7.766092
Train epoch: 8 [76800/98400 (78%)]	Loss: 7.958972
Train epoch: 8 [81920/98400 (83%)]	Loss: 7.818501
Train epoch: 8 [87040/98400 (88%)]	Loss: 7.796704
Train epoch: 8 [92160/98400 (93%)]	Loss: 7.785963
Train epoch: 8 [97280/98400 (98%)]	Loss: 7.765721
Make prediction for 19683 samples...
result: (0.327146, 0.5284412297132217)
Training on 98400 samples...
Total number of parameters: 4396164
Trainable parameters: 4396164
Train epoch: 9 [0/98400 (0%)]	Loss: 7.771965
Train epoch: 9 [5120/98400 (5%)]	Loss: 7.747793
Train epoch: 9 [10240/98400 (10%)]	Loss: 7.960270
Train epoch: 9 [15360/98400 (16%)]	Loss: 7.797658
Train epoch: 9 [20480/98400 (21%)]	Loss: 7.726526
Train epoch: 9 [25600/98400 (26%)]	Loss: 7.777538
Train epoch: 9 [30720/98400 (31%)]	Loss: 7.790020
Train epoch: 9 [35840/98400 (36%)]	Loss: 7.703180
Train epoch: 9 [40960/98400 (41%)]	Loss: 7.780016
Train epoch: 9 [46080/98400 (47%)]	Loss: 7.733831
Train epoch: 9 [51200/98400 (52%)]	Loss: 7.684304
Train epoch: 9 [56320/98400 (57%)]	Loss: 7.734194
Train epoch: 9 [61440/98400 (62%)]	Loss: 7.715170
Train epoch: 9 [66560/98400 (67%)]	Loss: 7.704819
Train epoch: 9 [71680/98400 (73%)]	Loss: 7.696820
Train epoch: 9 [76800/98400 (78%)]	Loss: 7.759115
Train epoch: 9 [81920/98400 (83%)]	Loss: 7.764024
Train epoch: 9 [87040/98400 (88%)]	Loss: 7.705262
Train epoch: 9 [92160/98400 (93%)]	Loss: 7.737270
Train epoch: 9 [97280/98400 (98%)]	Loss: 7.768048
Make prediction for 19683 samples...
result: (0.417724, 0.5312298391125485)
Training on 98400 samples...
Total number of parameters: 4396164
Trainable parameters: 4396164
Train epoch: 10 [0/98400 (0%)]	Loss: 7.760783
Train epoch: 10 [5120/98400 (5%)]	Loss: 7.717577
Train epoch: 10 [10240/98400 (10%)]	Loss: 7.868435
Train epoch: 10 [15360/98400 (16%)]	Loss: 7.689236
Train epoch: 10 [20480/98400 (21%)]	Loss: 7.658340
Train epoch: 10 [25600/98400 (26%)]	Loss: 7.688136
Train epoch: 10 [30720/98400 (31%)]	Loss: 7.634118
Train epoch: 10 [35840/98400 (36%)]	Loss: 7.639803
Train epoch: 10 [40960/98400 (41%)]	Loss: 7.782934
Train epoch: 10 [46080/98400 (47%)]	Loss: 7.636816
Train epoch: 10 [51200/98400 (52%)]	Loss: 7.651125
Train epoch: 10 [56320/98400 (57%)]	Loss: 7.671578
Train epoch: 10 [61440/98400 (62%)]	Loss: 7.604342
Train epoch: 10 [66560/98400 (67%)]	Loss: 7.658854
Train epoch: 10 [71680/98400 (73%)]	Loss: 7.609176
Train epoch: 10 [76800/98400 (78%)]	Loss: 7.677600
Train epoch: 10 [81920/98400 (83%)]	Loss: 7.603980
Train epoch: 10 [87040/98400 (88%)]	Loss: 7.648168
Train epoch: 10 [92160/98400 (93%)]	Loss: 7.652755
Train epoch: 10 [97280/98400 (98%)]	Loss: 7.661796
Make prediction for 19683 samples...
result: (0.3173906, 0.5228724041904254)

predicting for test data
Make prediction for 19683 samples...
result: (0.3173906, 0.5228724027318575)
